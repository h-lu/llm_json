# ==========================================
# 统一配置 (Unified Configuration)
# 复制此文件为 .env 并根据需要修改
# ==========================================

# --- 核心开关 ---
# 运行模式: 
#   'ONLINE_API'    -> 使用 DeepSeek 或其他 OpenAI 兼容 API (适合调试、复杂推理)
#   'LOCAL_OFFLINE' -> 使用本地 vLLM 引擎 (适合大规模、高吞吐、隐私数据)
EXECUTION_MODE=ONLINE_API

# 系统提示词 (支持多行，用 \n 表示换行)
SYSTEM_PROMPT=你是一名法律专家助手。请从提供的中国法院判决书中提取结构化数据。

# --- 生成参数 (Generation Parameters) ---
# 温度 (0.0 = 完全确定性, 1.0 = 平衡, 2.0 = 高度随机)
# 数据提取任务推荐 0.0，创意写作任务可调高
TEMPERATURE=0.0
# 最大输出 Token 数 (根据您的提取结果复杂度调整)
# 简单提取: 512-1024, 复杂提取: 2048-4096
MAX_OUTPUT_TOKENS=2048

# --- 重试配置 (Retry Configuration) ---
# 最大重试次数 (在线 API 模式)
MAX_RETRIES=3
# 重试延迟 (秒)
RETRY_DELAY_SECONDS=2

# ==========================================
# 模式 A: ONLINE_API 设置
# ==========================================
DEEPSEEK_API_KEY=sk-your-key-here
DEEPSEEK_BASE_URL=https://api.deepseek.com
ONLINE_MODEL_NAME=deepseek-chat

# ==========================================
# 模式 B: LOCAL_OFFLINE 设置 (vLLM)
# ==========================================
# 模型路径 (HuggingFace ID 或 本地绝对路径)
MODEL_PATH=Qwen/Qwen2.5-14B-Instruct

# 批量处理配置
BATCH_INPUT_DIR=data
BATCH_OUTPUT_DIR=data
# 每个 JSONL 分片的大小 (行数)
SHARD_SIZE=10000

# --- 硬件配置 ---
# 并行 GPU 数量 (启动多少个进程)
# 例如: 7张卡就设为 7，会启动 7 个 Worker 并行处理
NUM_GPUS=1

# 单个模型实例占用的显卡数量 (Tensor Parallelism)
# - 场景 A (推荐): 模型能装进单张显卡 (如 Qwen 14B + L40 48GB) -> 设为 1 (吞吐量最高)
# - 场景 B: 模型太大单卡装不下 (如 Qwen 72B + L40 48GB) -> 设为 2, 4 或 8 (必须是2的幂)
TENSOR_PARALLEL_SIZE=1

# 显存占用率 (0.90 - 0.95)
GPU_MEMORY_UTILIZATION=0.90

# 上下文长度限制 (防止 OOM)
MAX_MODEL_LEN=8192

# 开启前缀缓存 (加速 System Prompt)
ENABLE_PREFIX_CACHING=True

# --- vLLM 进阶性能参数 (可选) ---
# 最大并发序列数 (默认 256)。显存够大可调高，提高吞吐量。
# MAX_NUM_SEQS=256

# 每次迭代处理的最大 Token 数 (默认 2048)。
# 如果经常遇到 OOM，可以调小此值；如果显存有富余，可调大以加速。
# MAX_NUM_BATCHED_TOKENS=2048
